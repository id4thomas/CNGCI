{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import torch\n",
    "from transformers import (\n",
    "\tAutoTokenizer, AutoModelForCausalLM\n",
    ")\n",
    "\n",
    "from src.modules.loader import (\n",
    "\tload_subject_extractor,\n",
    "\tload_commonsense_generator,\n",
    "\tload_nli_predictor\n",
    ")\n",
    "from src.modules.commonsense_relation_generator import CATEGORIES\n",
    "from src.candidate_generator import ObsLM245NextSentenceCandidateGenerator\n",
    "from src.story_dataclasses import CommonsenseRelation, StorySentence, ConflictStory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dir = \"/home/ubuntu/yrsong/research/240711_cngci/weights/story_completion/roc_finetune_obs_lm_245_001\"\n",
    "device = torch.device(\"cuda\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Load Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "commonsense_generator = load_commonsense_generator(\n",
    "\tcomet_model_dir = \"/home/ubuntu/yrsong/research/240711_cngci/weights/comet-atomic_2020_BART\",\n",
    "\tembedding_model_dir = \"sentence-transformers/all-MiniLM-L6-v2\",\n",
    "\tdevice = device\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Make Sample Story"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_generator_batch_size = 32\n",
    "text_embedder_batch_size = 128\n",
    "decode_params = {\n",
    "\t\"num_beams\": 5,\n",
    "\t\"num_return_sequences\": 5\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/miniforge3/envs/yrsong_roc/lib/python3.8/site-packages/transformers/generation/utils.py:1355: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "## Initialize Sample Story\n",
    "context_sentence = StorySentence(\n",
    "\tidx = 0,\n",
    "\tvalue = \"Lana was trying to figure out how to play a song.\",\n",
    "\tcharacter = \"\",\n",
    "\tsentence_type = \"context\",\n",
    "\tcommonsense_relations = []\n",
    ")\n",
    "context_sentence.commonsense_relations = commonsense_generator.generate(\n",
    "\tcontext_sentence.value,\n",
    "\trelation_types = CATEGORIES,\n",
    "\tdecode_params = decode_params,\n",
    "\ttext_generator_batch_size = text_generator_batch_size,\n",
    "\ttext_embedder_batch_size = text_embedder_batch_size\n",
    ")\n",
    "\n",
    "obstacle_sentence = StorySentence(\n",
    "\tidx = 2,\n",
    "\tvalue = \"The song is very difficult.\",\n",
    "\tcharacter = \"\",\n",
    "\tsentence_type = \"obstacle\",\n",
    "\tcommonsense_relations = []\n",
    ")\n",
    "obstacle_sentence.commonsense_relations = commonsense_generator.generate(\n",
    "\tobstacle_sentence.value,\n",
    "\trelation_types = CATEGORIES,\n",
    "\tdecode_params = decode_params,\n",
    "\ttext_generator_batch_size = text_generator_batch_size,\n",
    "\ttext_embedder_batch_size = text_embedder_batch_size\n",
    ")\n",
    "\n",
    "## S2\n",
    "s2_sentence = StorySentence(\n",
    "\tidx = 1,\n",
    "\tvalue = \"For some reason, she couldn't figure out how to play the song.\",\n",
    "\tcharacter = \"\",\n",
    "\tsentence_type = \"other\",\n",
    "\tcommonsense_relations = []\n",
    ")\n",
    "s2_sentence.commonsense_relations = commonsense_generator.generate(\n",
    "\ts2_sentence.value,\n",
    "\trelation_types = CATEGORIES,\n",
    "\tdecode_params = decode_params,\n",
    "\ttext_generator_batch_size = text_generator_batch_size,\n",
    "\ttext_embedder_batch_size = text_embedder_batch_size\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "story = ConflictStory(\n",
    "\tnum_sentences = 3,\n",
    "\tcontext_idx = 0,\n",
    "\tobstacle_idx = 2,\n",
    "\tsentences = {\n",
    "\t\t0: context_sentence,\n",
    "\t\t1: s2_sentence,\n",
    "\t\t2: obstacle_sentence\n",
    "\t}\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Test Candidate Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Embedding(50257, 768)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(model_dir)\n",
    "model.resize_token_embeddings(len(tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator = ObsLM245NextSentenceCandidateGenerator(\n",
    "\tmodel = model,\n",
    "\ttokenizer = tokenizer,\n",
    "\tdevice = device\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "contexts = \"Lana was trying to figure out how to play a song.\"\n",
    "obstacles = \"The song is very difficult.\"\n",
    "# S2\n",
    "previous_sentences = [\"For some reason, she couldn't figure out how to play the song.\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Test Rules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.scorer import ImplicationRuleScorer, SimilarityRuleScorer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "coref model loaded.\n"
     ]
    }
   ],
   "source": [
    "## Load scorer\n",
    "subject_extractor = load_subject_extractor(model = \"en_core_web_sm\")\n",
    "nli_predictor = load_nli_predictor(\n",
    "\tmodel_dir = \"cross-encoder/nli-distilroberta-base\",\n",
    "\tdevice = device\n",
    ")\n",
    "rule_dir = \"rule_configs/comet_rule4.json\"\n",
    "with open(rule_dir, \"r\") as f:\n",
    "\trules = json.load(f)\n",
    "\n",
    "comet_decode_params = {\n",
    "\t\"num_beams\": 5,\n",
    "\t\"num_return_sequences\": 5\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "implication_scorer = ImplicationRuleScorer(\n",
    "\tnli_rules = rules[\"implication\"],\n",
    "\tweight_rules = rules[\"weights\"],\n",
    "\tsubject_extractor = subject_extractor,\n",
    "\tnli_predictor = nli_predictor,\n",
    "\tnli_predictor_batch_size = 128\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "similarity_scorer = SimilarityRuleScorer(\n",
    "\trules = rules[\"similarity\"],\n",
    "\tsubject_extractor = subject_extractor\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    }
   ],
   "source": [
    "story = ConflictStory(\n",
    "\tnum_sentences = 3,\n",
    "\tcontext_idx = 0,\n",
    "\tobstacle_idx = 2,\n",
    "\tsentences = {\n",
    "\t\t0: context_sentence,\n",
    "\t\t1: s2_sentence,\n",
    "\t\t2: obstacle_sentence\n",
    "\t}\n",
    ")\n",
    "\n",
    "decode_params = {\n",
    "\t\"num_beams\": 10,\n",
    "\t\"num_beam_groups\": 5,\n",
    "\t# \"temperature\": 0.9,\n",
    "\t# \"top_k\": 40,\n",
    "    \"num_return_sequences\": 5,\n",
    "\t\"repetition_penalty\": 10.0,\n",
    "\t# \"repetition_penalty\": 1.2,\n",
    "    \"diversity_penalty\": 10.0,\n",
    "\t\"max_new_tokens\": 128,\n",
    "\t\"early_stopping\": True\n",
    "}\n",
    "## sampling\n",
    "# decode_params = {\n",
    "#     \"num_return_sequences\": 5,\n",
    "# \t\"do_sample\": True,\n",
    "# \t# \"temperature\": 0.9,\n",
    "# \t# \"top_k\": 40,\n",
    "# \t# \"repetition_penalty\": 10.0,\n",
    "#     # \"diversity_penalty\": 100.0,\n",
    "# \t\"max_new_tokens\": 128,\n",
    "# \t\"early_stopping\": True\n",
    "# }\n",
    "\n",
    "candidates = generator.generate(story = story, decode_params = decode_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['She asked her friend for help.',\n",
       " 'She asked her friend for help.',\n",
       " 'Then her friend asked her for help.',\n",
       " 'Luckily, someone offered help and helped her practice.',\n",
       " 'Then her friend asked her for help.']"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "candidates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/miniforge3/envs/yrsong_roc/lib/python3.8/site-packages/transformers/generation/utils.py:1355: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "100%|██████████| 3/3 [00:00<00:00, 199.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NLI Predictor input 16\n",
      "NLI Predictor input 25\n",
      "NLI Predictor input 25\n",
      "NLI Score: 0.3333\n",
      "Weight: 0.3250\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4/4 [00:00<00:00, 200.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NLI Predictor input 20\n",
      "NLI Predictor input 25\n",
      "NLI Predictor input 20\n",
      "NLI Predictor input 20\n",
      "NLI Score: 0.2500\n",
      "Weight: 0.3875\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:00<00:00, 188.27it/s]\n",
      "/home/ubuntu/miniforge3/envs/yrsong_roc/lib/python3.8/site-packages/transformers/generation/utils.py:1355: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NLI Predictor input 20\n",
      "NLI Predictor input 20\n",
      "NLI Score: 0.5000\n",
      "Weight: 0.3000\n",
      "PAIRWISE (4, 5)\n",
      "PAIR oEffect - xNeed: 0.0000\n",
      "PAIRWISE (4, 5)\n",
      "PAIR oReact - xAttr: 0.0000\n",
      "PAIRWISE (4, 5)\n",
      "PAIR oWant - xIntent: 0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/3 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NLI Predictor input 16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:00<00:00, 195.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NLI Predictor input 25\n",
      "NLI Predictor input 25\n",
      "NLI Score: 0.3333\n",
      "Weight: 0.3250\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/4 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NLI Predictor input 20\n",
      "NLI Predictor input 25\n",
      "NLI Predictor input 20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4/4 [00:00<00:00, 204.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NLI Predictor input 20\n",
      "NLI Score: 0.2500\n",
      "Weight: 0.3875\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:00<00:00, 204.00it/s]\n",
      "/home/ubuntu/miniforge3/envs/yrsong_roc/lib/python3.8/site-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
      "  warnings.warn(\n",
      "/home/ubuntu/miniforge3/envs/yrsong_roc/lib/python3.8/site-packages/transformers/generation/utils.py:1355: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NLI Predictor input 20\n",
      "NLI Predictor input 20\n",
      "NLI Score: 0.5000\n",
      "Weight: 0.3000\n",
      "PAIRWISE (4, 5)\n",
      "PAIR oEffect - xNeed: 0.0000\n",
      "PAIRWISE (4, 5)\n",
      "PAIR oReact - xAttr: 0.0000\n",
      "PAIRWISE (4, 5)\n",
      "PAIR oWant - xIntent: 0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:00<00:00, 198.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NLI Predictor input 20\n",
      "NLI Predictor input 20\n",
      "NLI Score: 0.5000\n",
      "Weight: 0.3250\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4/4 [00:00<00:00, 203.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NLI Predictor input 20\n",
      "NLI Predictor input 25\n",
      "NLI Predictor input 20\n",
      "NLI Predictor input 20\n",
      "NLI Score: 0.2500\n",
      "Weight: 0.3875\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:00<00:00, 168.35it/s]\n",
      "/home/ubuntu/miniforge3/envs/yrsong_roc/lib/python3.8/site-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
      "  warnings.warn(\n",
      "/home/ubuntu/miniforge3/envs/yrsong_roc/lib/python3.8/site-packages/transformers/generation/utils.py:1355: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NLI Predictor input 20\n",
      "NLI Predictor input 20\n",
      "NLI Score: 0.0000\n",
      "Weight: 0.3000\n",
      "PAIRWISE (4, 5)\n",
      "PAIR oEffect - xNeed: 0.0000\n",
      "PAIRWISE (4, 5)\n",
      "PAIR oReact - xAttr: 0.0000\n",
      "PAIRWISE (4, 5)\n",
      "PAIR oWant - xIntent: 0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:00<00:00, 115.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NLI Predictor input 20\n",
      "NLI Predictor input 20\n",
      "NLI Score: 0.5000\n",
      "Weight: 0.3250\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4/4 [00:00<00:00, 111.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NLI Predictor input 16\n",
      "NLI Predictor input 25\n",
      "NLI Predictor input 20\n",
      "NLI Predictor input 20\n",
      "NLI Score: 0.0000\n",
      "Weight: 0.3875\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:00<00:00, 103.49it/s]\n",
      "/home/ubuntu/miniforge3/envs/yrsong_roc/lib/python3.8/site-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
      "  warnings.warn(\n",
      "/home/ubuntu/miniforge3/envs/yrsong_roc/lib/python3.8/site-packages/transformers/generation/utils.py:1355: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NLI Predictor input 20\n",
      "NLI Predictor input 20\n",
      "NLI Score: 0.5000\n",
      "Weight: 0.3000\n",
      "PAIRWISE (4, 5)\n",
      "PAIR oEffect - xNeed: 0.2000\n",
      "PAIRWISE (4, 5)\n",
      "PAIR oReact - xAttr: 0.2000\n",
      "PAIRWISE (4, 5)\n",
      "PAIR oWant - xIntent: 0.2000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:00<00:00, 110.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NLI Predictor input 20\n",
      "NLI Predictor input 20\n",
      "NLI Score: 0.5000\n",
      "Weight: 0.3250\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4/4 [00:00<00:00, 109.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NLI Predictor input 20\n",
      "NLI Predictor input 25\n",
      "NLI Predictor input 20\n",
      "NLI Predictor input 20\n",
      "NLI Score: 0.2500\n",
      "Weight: 0.3875\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:00<00:00, 113.61it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NLI Predictor input 20\n",
      "NLI Predictor input 20\n",
      "NLI Score: 0.0000\n",
      "Weight: 0.3000\n",
      "PAIRWISE (4, 5)\n",
      "PAIR oEffect - xNeed: 0.0000\n",
      "PAIRWISE (4, 5)\n",
      "PAIR oReact - xAttr: 0.0000\n",
      "PAIRWISE (4, 5)\n",
      "PAIR oWant - xIntent: 0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "scores = []\n",
    "\n",
    "candidate_idx = 3\n",
    "## Initialize Partial Story\n",
    "story.num_sentences = 4\n",
    "\n",
    "\n",
    "for candidate_value in candidates:\n",
    "\t## Make Candidate\n",
    "\tcandidate = StorySentence(\n",
    "\t\tidx = candidate_idx,\n",
    "\t\tvalue = candidate_value,\n",
    "\t\tcharacter = \"\",\n",
    "\t\tsentence_type = \"other\",\n",
    "\t\tcommonsense_relations = []\n",
    "\t)\n",
    "\tcandidate.commonsense_relations = commonsense_generator.generate(\n",
    "\t\tcandidate.value,\n",
    "\t\trelation_types = CATEGORIES,\n",
    "\t\tdecode_params = comet_decode_params,\n",
    "\t\ttext_generator_batch_size = text_generator_batch_size,\n",
    "\t\ttext_embedder_batch_size = text_embedder_batch_size\n",
    "\t)\n",
    "\tstory.sentences[candidate_idx] = candidate\n",
    "\n",
    "\t## Calculate Rule\n",
    "\timplication_context_score = implication_scorer.calculate_score(\n",
    "\t\tstory = story,\n",
    "\t\tcandidate_sentence_idx=candidate_idx,\n",
    "\t\tcomparing_sentence_type=\"context\"\n",
    "\t)\n",
    "\timplication_obstacle_score = implication_scorer.calculate_score(\n",
    "\t\tstory = story,\n",
    "\t\tcandidate_sentence_idx=candidate_idx,\n",
    "\t\tcomparing_sentence_type=\"obstacle\"\n",
    "\t)\n",
    "\timplication_preceding_score = implication_scorer.calculate_score(\n",
    "\t\tstory = story,\n",
    "\t\tcandidate_sentence_idx=candidate_idx,\n",
    "\t\tcomparing_sentence_type=\"preceding\"\n",
    "\t)\n",
    "\tsimilarity_score = similarity_scorer.calculate_score(\n",
    "\t\tstory = story,\n",
    "\t\tcandidate_sentence_idx=candidate_idx\n",
    "\t)\n",
    "\t\n",
    "\tscore = implication_context_score + implication_obstacle_score + implication_preceding_score + similarity_score\n",
    "\tscores.append(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SCORE: 0.355 She asked her friend for help.\n",
      "SCORE: 0.355 She asked her friend for help.\n",
      "SCORE: 0.259 Then her friend asked her for help.\n",
      "SCORE: 0.513 Luckily, someone offered help and helped her practice.\n",
      "SCORE: 0.259 Then her friend asked her for help.\n"
     ]
    }
   ],
   "source": [
    "for candidate, score in zip(candidates, scores):\n",
    "\tprint(\"SCORE: {:.3f} {}\".format(score, candidate))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "yrsong_roc",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
