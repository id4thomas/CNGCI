## Using devel image for flash-attention
FROM pytorch/pytorch:2.4.0-cuda12.1-cudnn9-devel

ENV CUDA_HOME="/usr/local/cuda"
RUN apt-get update; apt-get install -y curl zip unzip git;

## Install Requirements
COPY ./requirements.txt /workspace/requirements.txt
RUN pip install -r /workspace/requirements.txt
# Install flash attention
RUN pip install flash-attn==2.6.3 --no-build-isolation

## Default CMD
CMD ["/bin/bash"]