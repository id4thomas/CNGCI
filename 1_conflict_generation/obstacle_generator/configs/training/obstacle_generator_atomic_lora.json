{
	"data": {
		"data_dir": "data/defeasible-nli/data/defeasible-nli/defeasible-atomic",
		"train_fname": "train.jsonl",
		"val_fname": "dev.jsonl",
		"test_fname": "test.jsonl",
		"load":{
			"skip_impossible": true,
			"only_use_weakeners": true,
			"mode": "phu",
			"max_length": 256
		}
	},
	"training": {
		"run_name": "obstacle-generator-atomic-lora",
		"pretrained_model": "gpt2-xl",
		"optim": "adafactor",
		"learning_rate": 1e-5,
		"per_device_batch_size": 32,
		"gradient_accumulation_steps": 1,
		"label_smoothing_factor": 0,
		"save_strategy": "steps",
		"save_steps": 300,
		"save_total_limit": 3,
		"evaluation_strategy": "steps",
		"eval_steps": 20,
		"logging_strategy": "steps",
		"logging_steps": 20,
		"disable_tqdm": false,
		"report_to": ["wandb"],
		"seed": 100,
		"fp16": false,
		"bf16": true,
		"remove_unsued_columns": false,
		"load_best_model_at_end": true,
		"enable_flash_attn2": true,
		"enable_bettertransformer": false,
		"peft": {
			"lora_r": 8,
			"lora_alpha": 16,
			"lora_dropout": 0.05,
			"target_modules": ["c_attn"]
		}
	},
	"logging": {
		"weight_dir": "weights",
		"log_dir": "logs"
	}
}